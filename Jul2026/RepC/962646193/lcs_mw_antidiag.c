#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <mpi.h>
#include <pthread.h>
#include <papi.h>
#include "omp.h"

#define NUM_WORKER_THREADS 8        // Number of threads to use in each worker (uncomment row 763 to use this value)
#define TILE_DIM 5000               // Size of the block (tile) into which the DP matrix is divided
#define INNER_TILE_DIM 100          // Size of inner block (sub-tile) into which each tile received by each worker is divided

#define HOST_BUFF 256               // Size of the buffer for the hostname
#define TAG_TASK 0                  // Tag of the messages of type "task"
#define TAG_RESULT 1                // Tag of the messages of type "result"

/*
 * Task structure: represents a work unit to be processed by a worker.
 * Each task is associated with a specific tile of the DP matrix and contains information about:
 * - The task ID (i, j) representing the position of the tile in the matrix;
 * - The starting indices in the strings A and B;
 * - The size of the tile (tile_h, tile_w);
 * - The top and left edges of the matrix (top_row, left_col);
 * - The angle of the task (angle);
 * - Flags indicating the readiness of the edges (top_row_ready, left_col_ready, angle_ready);
 * - An initialization flag (initialized).
 */
typedef struct {
    int task_id[2];
    int start_index_sub_a;
    int start_index_sub_b;
    int tile_h;
    int tile_w;
    int top_row[TILE_DIM];
    int left_col[TILE_DIM];
    int angle;
    char top_row_ready;
    char left_col_ready;
    char angle_ready;
    char initialized;
} Task;

/*
 * Result structure: represents the output generated by a task and contains:
 * - The task ID (i, j) representing the position of the tile in the matrix;
 * - The right column (right_col) and bottom row (bottom_row) of the processed tile.
 */
typedef struct{
    int task_id[2];
    int right_col[TILE_DIM];
    int bottom_row[TILE_DIM];
} Result;

/*
* The idea of the tiling is to divide the DP matrix into blocks (tiles) of size TILE_DIM x TILE_DIM
* and send them to the workers. Each worker will process its assigned tile using OpenMP.
* 
* To compute each tile, the worker needs:
* - R: the top row of the tile above;
* - C: the left column of the tile to the left;
* - A: the angle, which is the last element of the tile above-left (diagonal).
* 
* E.g., for a tile of size 4x4, the dependencies are:
*
*  A   R   R   R   R
*    +---+---+---+---+ ^
*  C |   |   |   |   | | T
*    +---+---+---+---+ | I
*  C |   |   |   |   | | L
*    +---+---+---+---+ | E
*  C |   |   |   |   | | D
*    +---+---+---+---+ | I
*  C |   |   |   |   | | M
*    +---+---+---+---+ ⌄
*    <--------------->
*        TILE_DIM
*/

/*
 * Macro: INITIALIZE_TASK
 * --------------------
 * This macro initializes a task in the task_queue at the position indicated by task_counter.
 * It copies the fields of the "task" parameter into the task pointed by task_queue[task_counter],
 * including the task ID, starting indices, and tile dimensions.
 * It also inserts a memory barrier (__atomic_thread_fence) to ensure that the attribute "initialized"
 * is set only after all the other fields have been written.
 * This is important in a concurrent context, because a task can't be sent to a worker before all the fields are set,
 * so the attribute "initialized" is used to indicate that the task is ready to be sent.
 */
#define INITIALIZE_TASK(task, task_counter) { \
    task_queue[task_counter].task_id[0] = task.task_id[0]; \
    task_queue[task_counter].task_id[1] = task.task_id[1]; \
    task_queue[task_counter].start_index_sub_a = task.start_index_sub_a; \
    task_queue[task_counter].start_index_sub_b = task.start_index_sub_b; \
    task_queue[task_counter].tile_h = task.tile_h; \
    task_queue[task_counter].tile_w = task.tile_w; \
    __atomic_thread_fence(__ATOMIC_RELEASE); \
    task_queue[task_counter].initialized = 1; \
}

/* Macro: MAX
 * --------------------
 * Compare two values and return the maximum.
 */
#define MAX(a, b) ((a) > (b) ? (a) : (b))

/*
 * Macro: MIN
 * --------------------
 * Compare two values and return the minimum.
 */
#define MIN(a, b) ((a) < (b) ? (a) : (b))

/*
 * Prototypes of the functions called by the threads of the master process
 */
void *task_producer(void *args);
void *task_sender(void *args);
void *pending_task_sender(void *args);
void handle_PAPI_error(int, char*);

/*
 * Prototypes of the function called by the worker processes
 */
void lcs_tile_wavefront(Task *received_task_ptr);

/*
 * Global variables used by the master process and the workers
 */
int rank;                           // Rank of the process in the MPI_COMM_WORLD communicator
int max_antidiagonal_length;        // Max length of the antidiagonal (expressed in number of tiles)
int num_tiles_rows, num_tiles_cols; // Number of tiles by rows and columns
int string_lengths[2];              // Array to store the lengths of the two sequences to be compared for LCS
char *string_A, *string_B;          // Pointers to the two strings to be compared for LCS
char hn[HOST_BUFF];                 // Buffer to store the hostname of the machine on which the process is running

/*
 * Global variables used by the master process
 * --------------------
 * These variables are used for task coordination, concurrency management,
 * and communication between threads and processes (e.g., via MPI and pthreads).
 */
int num_tiles;                      // Total number of tile in the matrix
int num_antidiagonals;              // Total number of antidiagonals in the matrix
int *pending_task_indexes;            // Array of indices for the pending tasks (uninitialized tasks that are waiting to be sent to the workers)
int rank_worker;                    // Rank of the worker to which a task is sent
int max_rank_worker;                // Max rank of the worker (num_processes - 1)
int lcs_length = 0;                 // Length of the longest common subsequence (LCS)
int num_processes;                  // Number of processes in the MPI_COMM_WORLD communicator
char stop_auxiliary_sender;         // Flag to stop the pending task sender thread
pthread_mutex_t rank_worker_mutex;  // Mutex for the rank_worker variable, to ensure thread safety
Task *task_queue;                   // Array of tasks to be sent to the workers

/*
 * Global variables used by the worker processes
 * --------------------
 * These variables are used for the DP matrix and the computation of the LCS.
 */
int *DP_tile_elements;                       // Pointer to the first element of a contiguous block of memory allocated for all elements of the received tile
int **DP_tile;                    // Pointer pointing to the pointers pointing to the various rows of the received tile
int inner_tile_num;                 // Number of inner tiles (sub-tile) in the received tile
Result result;                      // Result structure to store the output of the task

/*
 * Function: main
 * --------------------
 * This is the main function of the program.
 * It contains both the code for the master process and the worker processes.
 * The master process is responsible for reading the input file, initializing the MPI environment,
 * creating the task queue, and managing the threads for task production and sending.
 * The worker processes are responsible for receiving tasks, processing them, and sending the results back to the master process.
 */
int main (int argc, char *argv[])
{
    // Each MPI process executes this section of code

    // Variables used to manage the MPI environment, PThread environment and PAPI library
    int provided;                   // Thread support level provided by MPI
    int rc;                         // Return code used for error handling
    int event_set = PAPI_NULL;      // Group of hardware events for PAPI library
    long_long num_cache_miss;       // To measure number of cache misses

    // MPI_Init_thread initializes the MPI environment and provides the requested threading level
    if ((rc = MPI_Init_thread(&argc, &argv, MPI_THREAD_SERIALIZED, &provided)) != MPI_SUCCESS)
    {
        fprintf(stderr, "MPI_Init error. Return code: %d\n", rc);
        exit(EXIT_FAILURE);
    }

    // Check if the provided threading level is at least MPI_THREAD_SERIALIZED
    if (provided < MPI_THREAD_SERIALIZED)
    {
        fprintf(stderr, "Minimum MPI threading level requested: %d (provided: %d)\n", MPI_THREAD_SERIALIZED, provided);
        exit(EXIT_FAILURE);
    }

    // PAPI setup
    if ((rc = PAPI_library_init(PAPI_VER_CURRENT)) != PAPI_VER_CURRENT)
        handle_PAPI_error(rc, "Error in library init.");
    if ((rc = PAPI_create_eventset(&event_set)) != PAPI_OK)
        handle_PAPI_error(rc, "Error while creating the PAPI eventset.");
    if ((rc = PAPI_add_event(event_set, PAPI_L2_TCM)) != PAPI_OK)
        handle_PAPI_error(rc, "Error while adding L2 total cache miss event.");
    if ((rc = PAPI_start(event_set)) != PAPI_OK) 
        handle_PAPI_error(rc, "Error in PAPI_start().");

    // MPI setup
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);           // Get the rank of the process in the MPI_COMM_WORLD communicator
    MPI_Comm_size(MPI_COMM_WORLD, &num_processes);  // Get the number of processes in the MPI_COMM_WORLD communicator
    gethostname(hn, HOST_BUFF);                     // Get the hostname of the machine on which the process is running
    max_rank_worker = num_processes - 1;            // Max rank of the worker (num_processes - 1)

    // The following code is executed only by the master process (rank 0)
    if (!rank)
    {
        int time_start, time_stop;  // Variables to measure elapsed time
        FILE *fp;                   // File pointer for the input file
        stop_auxiliary_sender = 0;  // Flag to stop the pending task sender thread
        
        // Initialize the mutex for the rank_worker variable
        rc = pthread_mutex_init(&rank_worker_mutex, NULL);
        if (rc)
        { 
            printf("PThread elements init error.\n");
            exit(-1);
        }

        // Create the thread IDs for the producer, sender, and auxiliary sender threads (pending task sender)
        pthread_t producer_thread, sender_thread, auxiliary_sender_thread;

        // Read the input file containing the two sequences to be compared for LCS
        // The file name is passed as a command line argument and neeeds to be in the format:
        // <len_a> <len_b>\n<string_A>\n<string_B>
        if (argc <= 1)
        {
            fprintf(stderr, "Error: No input file specified! Please specify the input file, and run again!\n");
            return 0;
        }
        
        // Open the input file for reading
        if ((fp = fopen(argv[1], "r")) == NULL)
        {
            fprintf(stderr, "Error while opening file.");
            exit(-1);
        }
        
        // Read the lengths of the two sequences from the file
        fscanf(fp, "%d %d", &string_lengths[0], &string_lengths[1]);

        // Print the lengths of the two sequences
        printf("(MASTER %d on %s) (thread %lu) Sequence lengths: %d %d\n", rank, hn, (unsigned long)pthread_self(), string_lengths[0], string_lengths[1]);

        // Allocate memory for the two strings
        string_A = malloc((string_lengths[0] + 1) * sizeof(char));
        string_B = malloc((string_lengths[1] + 1) * sizeof(char));

        // Read the two strings from the file
        fscanf(fp, "%s %s", string_A, string_B);

        // Close the input file
        fclose(fp);

        // Start the timer to measure elapsed time
        time_start = PAPI_get_real_usec();

        // Compute the number of tiles in each row, column and antidiagonal of the DP matrix
        num_tiles_rows = (string_lengths[0] + TILE_DIM - 1) / TILE_DIM;
        num_tiles_cols = (string_lengths[1] + TILE_DIM - 1) / TILE_DIM;
        num_antidiagonals = num_tiles_rows + num_tiles_cols - 1;

        // Compute the total number of tiles in the DP matrix
        num_tiles = num_tiles_rows * num_tiles_cols;

        // Compute the maximum length of the antidiagonal (expressed in number of tiles)
        max_antidiagonal_length = MIN(num_tiles_rows, num_tiles_cols);

        // Allocate memory for the task queue and the pending task index array
        task_queue = calloc(num_tiles, sizeof(Task));
        pending_task_indexes = calloc(max_antidiagonal_length + 1, sizeof(int));

        // Send the lengths of the two sequences and the two strings to the workers
        MPI_Bcast(string_lengths, 2, MPI_INT, 0, MPI_COMM_WORLD);
        MPI_Bcast(string_A, string_lengths[0] + 1, MPI_CHAR, 0, MPI_COMM_WORLD);
        MPI_Bcast(string_B, string_lengths[1] + 1, MPI_CHAR, 0, MPI_COMM_WORLD);

        // Create the threads for task generation, task sending, and pending task sending
        rc = pthread_create(&producer_thread, NULL, task_producer, NULL);
        rc |= pthread_create(&sender_thread, NULL, task_sender, NULL);
        rc |= pthread_create(&auxiliary_sender_thread, NULL, pending_task_sender, NULL);
        if (rc)
        { 
            fprintf(stderr, "PThread creation error. Return code: %d\n", rc);
            exit(EXIT_FAILURE);
        }

        // Wait for the threads to finish
        pthread_join(producer_thread, NULL);
        pthread_join(sender_thread, NULL);
        pthread_cancel(auxiliary_sender_thread);
        pthread_join(auxiliary_sender_thread, NULL);

        // Stop the timer and print the elapsed time
        time_stop = PAPI_get_real_usec();
        printf("(MASTER %d on %s) (thread %lu) (PAPI) Elapsed time: %lld us\n", rank, hn, (unsigned long)pthread_self(), (time_stop - time_start));
        
        // Print the length of the longest common subsequence (LCS)
        printf("(MASTER %d on %s) (thread %lu) (PAPI) LCS: %d\n", rank, hn, (unsigned long)pthread_self(), lcs_length);
        
        // Free the allocated memory for the strings and the task queue
        free(task_queue);
        free(pending_task_indexes);
    
    // The following code is executed only by the worker processes (rank > 0)
    } else {
        
        // Worker processes local variables
        Task received_task; // Task received from the master process
        MPI_Status status;  // Status of the MPI communication

        // Receive the lengths of the two sequences from the master process
        MPI_Bcast(string_lengths, 2, MPI_INT, 0, MPI_COMM_WORLD);
    
        // Allocate memory for the two strings
        string_A = malloc((string_lengths[0] + 1) * sizeof(char));
        string_B = malloc((string_lengths[1] + 1) * sizeof(char));

        // Check if memory allocation was successful
        if (!string_A || !string_B)
        {
            perror("Alloc string buffers");
            exit(EXIT_FAILURE);
        }
    
        // Receive the two strings from the master process
        MPI_Bcast(string_A, string_lengths[0] + 1, MPI_CHAR, 0, MPI_COMM_WORLD);
        MPI_Bcast(string_B, string_lengths[1] + 1, MPI_CHAR, 0, MPI_COMM_WORLD);

        // Allocate a contiguous block of memory to contains all elements of the DP tile
        DP_tile_elements  = malloc((TILE_DIM + 1) * (TILE_DIM + 1) * sizeof(int)); 

        // Array of pointers to the rows of the DP tile
        DP_tile = malloc((TILE_DIM + 1) * sizeof(int*));
        if (!DP_tile)
        {
            perror("Alloc DP_matrix pointers");
            exit(EXIT_FAILURE);
        }

        /*
         * NOTE:
         * The idea is to store the DP tile in a contiguous block of memory and then create an array of pointers to the rows of the tile.
         * This way, we can access the elements of the tile using the DP_tile[i][j] notation,
         * while the actual memory is allocated in a single block, which is more efficient for cache usage.
         *
         * **DP_tile_elements (array of pointers to the rows of the DP tile):
         * +---------+---------+---------+---------+--------------+
         * |  DP[0]  |  DP[1]  |  DP[2]  |   ...   | DP[TILE_DIM] |
         * +---------+---------+---------+---------+--------------+
         *      |          |
         *      v          -------------------------------------|
         * *DP_tile (contiguous block of memory):               v
         * +----------+----------+---------+-----------------+----------+---------+---------+--------------+
         * | DP[0][0] | DP[0][1] |   ...   | DP[0][TILE_DIM] | DP[1][0] |   ...   | DP[TILE_DIM][TILE_DIM] |
         * +----------+----------+---------+-----------------+----------+---------+---------+--------------+
         */

        // Make each pointers of the DP_tile point to the correct element of the DP_tile_elements array
        // in order to make each DP_tile[i] point to the i-th row of the tile
        for (int i = 0; i < TILE_DIM + 1; ++i)
        {
            DP_tile[i] = DP_tile_elements + i * (TILE_DIM + 1);
        }

        // Initialize the DP_tile with zeros.
        // This operation is not necessary, but it allows the "first touch"
        // of the memory pages, which is useful for performance reasons
        // and to avoid false sharing in the OpenMP threads.
        #pragma omp parallel for schedule(static)
        for (int i = 0; i < TILE_DIM + 1; ++i)
        {
            memset(DP_tile[i], 0, (TILE_DIM + 1) * sizeof(int));
        }
    
        // The worker processes will receive tasks from the master process and process them calling the lcs_tile_wavefront function.
        while (1)
        {   
            // Receive a task from the master process
            MPI_Recv(&received_task, sizeof(Task), MPI_BYTE, 0, TAG_TASK, MPI_COMM_WORLD, &status);

            // Check if the received task is a termination signal (angle == -1)
            // If so, break the loop and terminate the worker process
            if (received_task.angle == -1) break;

            // Otherwise, process the received task
            lcs_tile_wavefront(&received_task);
        }
        
        // Free the allocated memory for the DP tile
        free(DP_tile_elements);
        free(DP_tile);
    }
   
    // Stop the PAPI library
    if ((rc = PAPI_stop(event_set, &num_cache_miss)) != PAPI_OK)
        handle_PAPI_error(rc, "Error in PAPI_stop().");
    
    // Print the number of cache misses for each rank
    printf("Rank: %d, total L2 cache misses:%lld\n", rank, num_cache_miss);
    
    // Terminate the MPI environment
    MPI_Finalize();

    // Free the allocated memory for the strings
    free(string_A);
    free(string_B);

    return 0;
}

/*
 * Function: task_producer
 * ----------------------
 * This function is executed by the the producer_thread created by the master process
 * and is responsible for generating tasks to be sent to the worker processes.
 * It iterates over the antidiagonals of the DP matrix, creating tasks for each tile in the matrix.
 * Each task is initialized and added to the task queue.
 * The function also sends the first task to the worker process with rank 1.
 */
void *task_producer (void *args)
{
    // Variables used to manage the task production
    MPI_Request send_request;
    Task task;

    // Task counter to keep track of the number of tasks produced
    // and to access to the right position in the task queue
    int task_counter = 0;

    // Variables declaration for the loop
    int d, i_start, i_end, i, j;

    // For each antidiagonal in the entire DP matrix
    for (d = 0; d < num_antidiagonals; d++)
    {
        // Compute the range of rows to consider for the current antidiagonal:
        // i_start and i_end are the start and end indices of the rows to consider
        i_start = (d < num_tiles_cols) ? 0 : d - num_tiles_cols + 1;
        i_end = (d < num_tiles_rows) ? d : num_tiles_rows - 1;

        // Iterate over the rows in the range [i_start, i_end]
        // to create the tasks for the current antidiagonal
        for (i = i_start; i <= i_end; i++)
        {
            // Compute the column index j for the current task
            // Since i + j = d, we can compute j as d - i
            // i is the row index and j is the column index of the tile in the DP matrix
            j = d - i;

            // Initialize the current task with the appropriate values:

            // task_id is an array of two integers (i, j) representing the position of the tile in the matrix
            task.task_id[0] = i; task.task_id[1] = j;

            // start_index_sub_a and start_index_sub_b are the starting indices in the strings A and B for the current tile
            task.start_index_sub_a = i * TILE_DIM;
            task.start_index_sub_b = j * TILE_DIM;

            // tile_h and tile_w are the height and width of the tile
            // They are computed as the minimum of TILE_DIM and the remaining length of the strings A and B
            // because the DP matrix may not be a perfect multiple of TILE_DIM
            // and the last tile may be smaller than TILE_DIM
            task.tile_h = MIN( TILE_DIM, string_lengths[0] - task.start_index_sub_a );
            task.tile_w = MIN( TILE_DIM, string_lengths[1] - task.start_index_sub_b );

            // Use the macro INITIALIZE_TASK to initialize the task in the task queue
            INITIALIZE_TASK(task, task_counter);

            // Increment the task counter for the next iteration
            task_counter++;
        }

        // If the current antidiagonal is the first one (d == 0),
        // send the first task to the worker process with rank 1 to start the computation
        // The first task is the one at the top-left corner of the DP matrix (i = 0, j = 0)
        if (!d)
        {   
            MPI_Isend(&task_queue[0], sizeof(Task), MPI_BYTE, 1, TAG_TASK, MPI_COMM_WORLD, &send_request);
        }
    }

    // Wait for the send request of the first task to complete
    // This is important to ensure that the first task is sent before the producer thread exits
    MPI_Wait(&send_request, MPI_STATUS_IGNORE);

    // Explicit exit of the thread
    pthread_exit(NULL);
}

/*
 * Function: task_sender
 * ----------------------
 * This function is executed by the sender_thread created by the master process
 * and is responsible for sending tasks to the worker processes.
 * It receives results from the workers and sends new tasks based on the dependencies of the received results.
 * The function also manages the pending tasks whose dependencies have been injected,
 * but have not yet been initialised by the producer_thread.
 * It uses a mutex to ensure that the rank_worker variable is accessed in a thread-safe manner.
 * The function runs in a loop until the stop_sender flag is set to true.
 */
void *task_sender(void *args) { // Funzione di thread 

    // Variables used to store the results received from the workers
    Result result;

    // Variables declaration for the loop
    int i, j, index_right, index_down, index_right_down;

    // Variables used to manage the task sending
    MPI_Status status;
    MPI_Request req;

    // Counter used to store the indices of the pending tasks in the pending_task_indexes array
    int count_pending_task = 0;

    // Flag to stop the sender thread
    // This flag is set to true when the last task (tile [TILE_DIM][TILE_DIM]) has been sent and the sender thread can exit
    char stop_sender = 0;
    
    // Loop until the stop_sender flag is set to true
    while (!stop_sender)
    {
        // Receive the result from the worker process
        MPI_Recv(&result, sizeof(Result), MPI_BYTE, MPI_ANY_SOURCE, TAG_RESULT, MPI_COMM_WORLD, &status);

        // Recover the indices i and j of the tile for which the result was received
        i = result.task_id[0];
        j = result.task_id[1];
        
        // Set the rank_worker variable to the rank of the worker that sent the result
        pthread_mutex_lock(&rank_worker_mutex);
        rank_worker = status.MPI_SOURCE;
        pthread_mutex_unlock(&rank_worker_mutex);

        // Check if the result obtained is not relative to a tile in the last column (j < N - 1)
        // In fact, if the result is relative to a tile in the last column, it means that there is no tile to the right of this tile for which we must set the left column
        if (j < num_tiles_cols - 1)
        {
            // Recover the index of the task_queue array where the tile to the right of the received one is located
            // The tile to the right is at (i, j + 1)
            index_right = tile_index(i, j + 1, num_tiles_rows, num_tiles_cols);
            
            // Inject the dependency in the task by copying the right column of the result into the left column of the task to the right
            memcpy(task_queue[index_right].left_col, result.right_col, sizeof(result.right_col));
            task_queue[index_right].left_col_ready = 1;

            // Check if the task has all the other dependencies besides the one just injected, i.e., if the top row and angle are ready
            if (!i || (task_queue[index_right].top_row_ready && task_queue[index_right].angle_ready))
            {
                // If the task is ready, check if it has been initialized by the producer_thread
                if (task_queue[index_right].initialized) {

                    // If the task is initialized, send it to the worker process
                    pthread_mutex_lock(&rank_worker_mutex);
                    // The increment of the rank_worker variable is done before sending the task
                    // So, the task is sent to the next worker, not to the same one that sent the result.
                    // Initially, each task was sent to the worker who had sent the result, since it was certainly free and available,
                    // but, counter-intuitively to what one might think, adopting this strategy allowed us to achieve better performance
                    rank_worker = (rank_worker == max_rank_worker) ? 1 : ++rank_worker;
                    MPI_Isend(&task_queue[index_right], sizeof(Task), MPI_BYTE, rank_worker, TAG_TASK, MPI_COMM_WORLD, &req);
                    MPI_Request_free(&req);
                    pthread_mutex_unlock(&rank_worker_mutex);
                }
                else
                {
                    // If the task is not initialized, add it to the pending task indexes array and let the auxiliary_sender_thread manage it
                    pending_task_indexes[count_pending_task] = index_right;
                    count_pending_task = (count_pending_task == (max_antidiagonal_length)) ? 0 : ++count_pending_task;
                }
            }
        }
    
        // Check if the result obtained is not relative to a tile in the last row (i < M - 1)
        // In fact, if the result is relative to a tile in the last row, it means that there is no tile at the bottom of this tile for which we have to set the top row
        if (i < num_tiles_rows - 1)
        {
            // Recover the index of the task_queue array where the tile below the received one is located
            // The tile below is at (i + 1, j)
            index_down = tile_index(i + 1, j, num_tiles_rows, num_tiles_cols);
            
            // Inject the dependency in the task by copying the bottom row of the result into the top row of the task below
            memcpy(task_queue[index_down].top_row, result.bottom_row, sizeof(result.bottom_row));
            task_queue[index_down].top_row_ready = 1;

            // Check if the task has all the other dependencies besides the one just injected, i.e., if the left column and angle are ready
            if (!j || (task_queue[index_down].left_col_ready && task_queue[index_down].angle_ready))
            {
                // If the task is ready, check if it has been initialized by the producer_thread
                if (task_queue[index_down].initialized) {

                    // If the task is initialized, send it to the worker process
                    pthread_mutex_lock(&rank_worker_mutex);
                    rank_worker = (rank_worker == max_rank_worker) ? 1 : ++rank_worker; // Incremento del rank del worker a cui inviare il messaggio
                    MPI_Isend(&task_queue[index_down], sizeof(Task), MPI_BYTE, rank_worker, TAG_TASK, MPI_COMM_WORLD, &req);
                    MPI_Request_free(&req);
                    pthread_mutex_unlock(&rank_worker_mutex); // Esce dalla sezione critica
                }
                else
                {
                    // If the task is not initialized, add it to the pending task indexes array and let the auxiliary_sender_thread manage it
                    pending_task_indexes[count_pending_task] = index_down;
                    count_pending_task = (count_pending_task == (max_antidiagonal_length)) ? 0 : ++count_pending_task;
                }
            }
        }

        // Check if the result obtained is not relative to a tile in the last row and last column [i != (M - 1) && j != (N - 1)]
        // In fact, if the result is for a tile in the last row or column, it means that there is no tile in the bottom right-hand corner of this tile for which we must set the correct angle value
        if (j != (num_tiles_cols - 1) && i != (num_tiles_rows - 1))
        {
            // Recover the index of the task_queue array where the tile in the bottom right corner of the received one is located
            // The tile in the bottom right corner is at (i + 1, j + 1)
            index_right_down = tile_index(i + 1, j + 1, num_tiles_rows, num_tiles_cols);

            // Inject the dependency in the task by copying the last element at the bottom right of the tile into the angle of the task in the bottom right corner
            task_queue[index_right_down].angle = result.bottom_row[TILE_DIM - 1];
            task_queue[index_right_down].angle_ready = 1;
        }
        // Otherwise, if the result is for the last tile in the DP matrix (bottom right corner),
        // set the lcs_length variable to the last element of the bottom row of the tile
        // This is the length of the longest common subsequence (LCS) between the two strings
        else if (j == num_tiles_cols - 1 && i == num_tiles_rows - 1)
        {
            // It is not certain that the last tile is of size TILE_DIM x TILE_DIM,
            // because the DP matrix may not be a perfect multiple of TILE_DIM
            // So, we need to compute the last width of the tile in the last row
            int last_column_width = string_lengths[1] - j * TILE_DIM;
            lcs_length = result.bottom_row[last_column_width - 1];
            stop_sender = 1; // Set the stop_sender flag to true to exit the loop
        }
    }

    // Set the stop_auxiliary_sender flag to 1 to stop the auxiliary_sender_thread
    stop_auxiliary_sender = 1;

    // After all tasks have been produced, create a task to send the termination signal to the worker processes
    // The termination signal is a task with angle == -1
    Task task;
    task.angle = -1;

    // Send the termination signal to all worker processes
    for (int k=1; k<=max_rank_worker; k++)
    {
        MPI_Send(&task, sizeof(Task), MPI_BYTE, k, TAG_TASK, MPI_COMM_WORLD); // Invio il messaggio di stop a tutti i worker
    }
    
    // Explicit exit of the thread
    pthread_exit(NULL);
}

/*
 * Function: pending_task_sender
 * ----------------------
 * This function is executed by the auxiliary_sender_thread created by the master process
 * and is responsible for sending the pending tasks to the worker processes.
 * It iterates over the pending task indices and sends the tasks to the workers as soon as they are ready.
 * The function uses a mutex to ensure thread safety when accessing the rank_worker variable.
 */
void *pending_task_sender (void *args)
{
    // Variable used to access the pending_task_indexes queue
    int c = 0;

    // The loop iterates over the pending task indices and sends the tasks to the workers as soon as they are ready
    // The loop continues until the stop_auxiliary_sender flag is set to 1
    while (1)
    {
        // Wait until a pending task index is available
        // All the pending_task_indexes elements are initialized to 0 at the beginning and when the sender_thread process a task
        // that is not ready, it sets a pending_task_indexes element to the index of the task in the task_queue
        // So, the loop will wait until a non-zero index is found in the pending_task_indexes array
        // This is an example of busy waiting, which is not the best practice in a real-world application
        // but is used here because we suppose that this thread will be executed on a different core
        while (!pending_task_indexes[c])
        {   
            // Check if the stop_auxiliary_sender flag is set to 1
            // If so, exit the loop and terminate the thread
            if (stop_auxiliary_sender) goto exit;
        }

        // Wait in busy waiting until the task at the pending_task_indexes[c] index is initialized
        while(!task_queue[pending_task_indexes[c]].initialized) { }

        // When the task is initialized and is ready to be sent,
        // acquire the mutex to ensure thread safety when accessing the rank_worker variable
        // and send the task to the worker process with the rank = rank_worker
        // The rank_worker variable is incremented to send the task to the next worker process in a round-robin fashion
        pthread_mutex_lock(&rank_worker_mutex);
        MPI_Send(&task_queue[pending_task_indexes[c]], sizeof(Task), MPI_BYTE, rank_worker, TAG_TASK, MPI_COMM_WORLD);
        rank_worker = (rank_worker == max_rank_worker) ? 1 : ++rank_worker;
        pthread_mutex_unlock(&rank_worker_mutex);

        // Reset the pending task index to 0, because the pending_task_indexes array is a circular buffer
        // So, the position [c] could be reused to store a new pending task index in the next iterations
        pending_task_indexes[c] = 0;

        // Increment the counter c to access the next pending task index in the next iteration
        // The counter is reset to 0 when it reaches the maximum length of the antidiagonal
        c = (c == (max_antidiagonal_length)) ? 0 : ++c;

        // The array pending_task_indexes has a maximum length of max_antidiagonal_length + 1
        // because, in the worst case, all the tasks in the current antidiagonal are pending
    }

    // Explicit exit of the thread
    exit:
    pthread_exit(NULL);
}

/*
 * Function: lcs_tile_wavefront
 * ----------------------
 * This function is executed by the worker processes and is responsible for computing the LCS for a given tile in the DP matrix.
 * Each worker, when it receives a task from the master process, make another level of parallelism by dividing the tile into smaller blocks (INNER_TILE_DIM x INNER_TILE_DIM)
 * and processing each block in parallel using OpenMP.
 * So, it uses a wavefront algorithm to compute the sub-tile of the DP matrix proceeding by anti-diagonals,
 * but for the computation of each sub-tile, each thread uses the classic dynamic programming algorithm.
 */
void lcs_tile_wavefront(Task *t) {

    // Variables declaration for the loop
    int d, min_row_index, max_row_index, sub_tile_i, sub_tile_j, st_start_row_index, st_end_row_index, st_start_col_index, st_end_col_index, up, left, char_index_str_A, char_index_str_B, i, j;
    
    // Set the angle dependecy of the tile to compute to the value of the angle of the received task
    DP_tile[0][0] = t->angle;

    // Variables used to store the dimensions of the tile
    int tile_height = t->tile_h;
    int tile_width = t->tile_w;

    // Set the top row and left column of the tile to compute to the values of the received task
    for (int k = 1; k <= tile_width; ++k)
        DP_tile[0][k] = t->top_row[k-1];
    for (int k = 1; k <= tile_height; ++k)
        DP_tile[k][0] = t->left_col[k-1];

    // The tile to compute is divided into blocks of size INNER_TILE_DIM×INNER_TILE_DIM
    // But the tile may not be a perfect multiple of INNER_TILE_DIM,
    // so we need to compute the number of blocks in each direction (vertical and horizontal)
    int tile_count_vertical = (tile_height + INNER_TILE_DIM - 1) / INNER_TILE_DIM;
    int tile_count_horizontal = (tile_width + INNER_TILE_DIM - 1) / INNER_TILE_DIM;

    // Here we compute the number of antidiagonals in the tile to use the for loop and proceed with the wavefront algorithm
    int antidiagonal_count = tile_count_vertical + tile_count_horizontal - 1;

    // For each antidiagonal in the tile, we compute the blocks that belong to that antidiagonal
    for (d = 0; d < antidiagonal_count; ++d)
    {
        // For each antidiagonal, we compute the minimum and maximum row index of the blocks that belong to that antidiagonal
        min_row_index = MAX(0, d - (tile_count_horizontal - 1));
        max_row_index = MIN(d, tile_count_vertical - 1);

        // Here we set the number of threads to use for the OpenMP parallel region
        // omp_set_num_threads(NUM_WORKER_THREADS);

        // Compute the blocks that belong to the current antidiagonal using OpenMP
        // The schedule is dynamic with a chunk size of 1, so each thread will process one block at a time
        // In the loop we iterate over the rows of the blocks that belong to the current antidiagonal
        #pragma omp parallel for schedule(dynamic, 1) private(sub_tile_i, sub_tile_j, st_start_row_index, st_end_row_index, st_start_col_index, st_end_col_index, i, j, up, left, char_index_str_A, char_index_str_B)
        for (sub_tile_i = min_row_index; sub_tile_i <= max_row_index; ++sub_tile_i) {
            sub_tile_j = d - sub_tile_i;

            // sub_tile_i and sub_tile_j are the indices of the block in the tile on which we are working
            // For example, if the tile is 4x4 and INNER_TILE_DIM = 2, we have:           
            // Sub-tile (0, 1) -> sub_tile_i = 0, sub_tile_j = d - 0 = 1 - 0 = 1

            // Now we need to compute the local indices of the singles elements of the individual elements of the sub-tile
            // First we compute the starting and ending indices of the rows of the elements in the sub-tile (st)
            int st_start_row_index = sub_tile_i * INNER_TILE_DIM + 1;
            int st_end_row_index = MIN((sub_tile_i + 1) * INNER_TILE_DIM, tile_height);

            // Then we compute the starting and ending indices of the columns of the elements in the sub-tile (st)
            int st_start_col_index = sub_tile_j * INNER_TILE_DIM + 1;
            int st_end_col_index = MIN((sub_tile_j + 1) * INNER_TILE_DIM, tile_width);

            //                st_start_col_index            st_end_col_index
            //                        |                             |
            //                        v                             v
            // st_start_row_index ->  +-----------------------------+   
            //                        |                             |   
            //                        |                             |   
            //                        |       SUB-TILE BLOCK        |   -->   dimensions: maximum INNER_TILE_DIM x INNER_TILE_DIM
            //                        |                             |       (could be smaller if the tile is not a perfect multiple of INNER_TILE_DIM)
            //                        |                             |   
            // st_end_row_index   ->  +-----------------------------+
            //                        ^                             ^
            //                        |                             |
            //        sub_tile_i * INNER_TILE_DIM      MIN((sub_tile_i+1)*INNER_TILE_DIM, tile_height)

            // For each element in the sub-tile, we compute the DP matrix using the classic dynamic programming algorithm
            for (int i = st_start_row_index; i <= st_end_row_index; ++i)
            {
                for (int j = st_start_col_index; j <= st_end_col_index; ++j)
                {
                    up = DP_tile[i-1][j]; // Element on top of the current element
                    left = DP_tile[i][j-1]; // Element on the left of the current element
                    char_index_str_A = t->start_index_sub_a + i - 1; // Index of the character in the string A to consider for the current element
                    char_index_str_B = t->start_index_sub_b + j - 1; // Index of the character in the string B to consider for the current element
                    
                    // If string_A[*] == string_B[*]:
                    DP_tile[i][j] = (string_A[char_index_str_A] == string_B[char_index_str_B])
                        ? DP_tile[i-1][j-1] + 1 // then DP[i][j] = DP[i-1][j-1] + 1
                        : MAX(up, left); // Otherwise DP[i][j] = max(DP[i-1][j], DP[i][j-1])
                }
            }
       }
    }

    // The wavefront algorithm has computed the tile, now we need to send the result back to the master process
    Result res;
    res.task_id[0] = t->task_id[0];
    res.task_id[1] = t->task_id[1];

    // It is necessary to send only the last row and last column of the tile to the master process
    for (int i = 1; i <= tile_height; ++i)
        res.right_col[i-1] = DP_tile[i][tile_width];
    for (int j = 1; j <= tile_width; ++j)
        res.bottom_row[j-1] = DP_tile[tile_height][j];
    
    MPI_Send(&res, sizeof(res), MPI_BYTE, 0, TAG_RESULT, MPI_COMM_WORLD);
}

/*
 * Function: length_of_diagonal
 * ----------------------
 * Given an anti‐diagonal index `d` in an M×N tile grid, compute how many tiles lie on
 * that diagonal. 
 *
 * Parameters:
 *   d – the 0-based anti-diagonal index (sum of row+column)
 *   M – number of tile rows
 *   N – number of tile columns
 *
 * Returns:
 *   The number of valid (i,j) pairs with i+j == d, i in [0,M), j in [0,N). 
 *   Returns 0 if `d` is outside [0,(M-1)+(N-1)].
 */
int length_of_diagonal(int d, int M, int N) {
    int start = MAX(0, d - (N - 1)); // First valid row index for diagonal d
    int end   = MIN(d, M - 1); // Last valid row index for diagonal d
    return (end >= start ? end - start + 1 : 0); // Number of valid (i,j) pairs on diagonal d
}

/*
 * Function: sum_of_lengths
 * ----------------------
 * Compute the total number of tiles in all anti-diagonals from 0 up to (but not including) `d`.
 * Internally calls length_of_diagonal(k,M,N) for each k in [0,d).
 *
 * Parameters:
 *   d – the number of anti-diagonals to sum over
 *   M – number of tile rows
 *   N – number of tile columns
 *
 * Returns:
 *   Sum_{k=0..d-1} length_of_diagonal(k,M,N).
 */
int sum_of_lengths(int d, int M, int N) {
    int sum = 0; 
    for (int k = 0; k < d; k++) {
        sum += length_of_diagonal(k, M, N);
    }
    return sum;
}

/*
 * Function: tile_index
 * ----------------------
 * Map a tile’s 2D coordinates (x, y) in an M×N grid into a linear index based on
 * anti-diagonal ordering. The grid is flattened by traversing anti-diagonals in
 * increasing order of (i+j), and within each diagonal by increasing row index.
 *
 * Parameters:
 *   x – tile row index (0..M-1)
 *   y – tile column index (0..N-1)
 *   M – number of tile rows
 *   N – number of tile columns
 *
 * Returns:
 *   A unique index in [0, M*N), equal to:
 *     sum_of_lengths(x+y, M, N)  // tiles in all earlier diagonals
 *     + (x – max(0, (x+y)-(N-1))) // offset within the current diagonal
 */
int tile_index(int x, int y, int M, int N) {
    int d = x + y;
    int before = sum_of_lengths(d, M, N); // Tiles in all earlier diagonals
    int r_start = MAX(0, d - (N - 1)); // First valid row index for diagonal d
    int offset = x - r_start; // Offset within the current diagonal: difference between the current row index and the first valid row index for diagonal d
    return before + offset;
}

// Function to handle PAPI errors. It prints the error message and exits the program.
void handle_PAPI_error(int rc, char *msg) {
    char error_str[PAPI_MAX_STR_LEN];
    memset(error_str, 0, sizeof(char)*PAPI_MAX_STR_LEN);
  
    fprintf(stderr, "%s\nReturn code: %d - PAPI error message:\n", msg, rc);
    PAPI_perror(error_str); PAPI_strerror(rc);
    exit(EXIT_FAILURE);
}