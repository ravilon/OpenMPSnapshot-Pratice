/**
 *  @brief  Low-latency OpenMP-style NUMA-aware cross-platform fine-grained parallelism library.
 *  @file   lib.cpp
 *  @author Ash Vardanian
 *  @date   June 27, 2025
 */
#include <fork_union.h>   // C type aliases
#include <fork_union.hpp> // C++ core implementation

#include <variant> // `std::variant`
#include <utility> // `std::in_place_type_t`

namespace fu = ashvardanian::fork_union;

using thread_allocator_t = std::allocator<std::thread>;

using pool_variants_t = std::variant< //

#if _FU_WITH_ASM_YIELDS
#if _FU_DETECT_ARCH_X86_64
    fu::basic_pool<thread_allocator_t, fu::x86_pause_t>,  //
    fu::basic_pool<thread_allocator_t, fu::x86_tpause_t>, //
#endif
#if _FU_DETECT_ARCH_ARM64
    fu::basic_pool<thread_allocator_t, fu::arm64_yield_t>, //
    fu::basic_pool<thread_allocator_t, fu::arm64_wfet_t>,  //
#endif
#if _FU_DETECT_ARCH_RISC5
    fu::basic_pool<thread_allocator_t, fu::risc5_pause_t>, //
#endif
#endif // _FU_WITH_ASM_YIELDS

#if FU_ENABLE_NUMA
    fu::linux_distributed_pool<fu::standard_yield_t>, //
#if _FU_WITH_ASM_YIELDS
#if _FU_DETECT_ARCH_X86_64
    fu::linux_distributed_pool<fu::x86_pause_t>,  //
    fu::linux_distributed_pool<fu::x86_tpause_t>, //
#endif
#if _FU_DETECT_ARCH_ARM64
    fu::linux_distributed_pool<fu::arm64_yield_t>, //
    fu::linux_distributed_pool<fu::arm64_wfet_t>,  //
#endif
#if _FU_DETECT_ARCH_RISC5
    fu::linux_distributed_pool<fu::risc5_pause_t>, //
#endif
#endif // _FU_WITH_ASM_YIELDS
#endif // FU_ENABLE_NUMA

    fu::basic_pool<thread_allocator_t, fu::standard_yield_t> //
    >;

struct opaque_pool_t {
    pool_variants_t variants;
    fu_lambda_context_t current_context; /// Current context for the unsafe callbacks
    fu_for_threads_t current_callback;   /// Current callback for the unsafe callbacks

    template <typename pool_type_, typename... args_types_>
    opaque_pool_t(std::in_place_type_t<pool_type_> inplace, args_types_ &&...args) noexcept
        : variants(inplace, std::forward<args_types_>(args)...), current_context(nullptr), current_callback(nullptr) {}

    /** @brief A shim to redirect unsafe callbacks to the current context. */
    void operator()(fu::colocated_thread_t pinned) const noexcept {
        current_callback(current_context, pinned.thread, pinned.colocation);
    }
};

static bool global_initialized {false};
static fu::numa_topology_t global_numa_topology {};
static fu::capabilities_t global_capabilities {fu::capabilities_unknown_k};
static char global_capabilities_string[128] {};

bool globals_initialize(void) {
    if (global_initialized) return true;

#if FU_ENABLE_NUMA
    if (!global_numa_topology.try_harvest()) return false;
#endif

    fu::capabilities_t cpu_caps = fu::cpu_capabilities();
    fu::capabilities_t ram_caps = fu::ram_capabilities();

    global_capabilities = static_cast<fu::capabilities_t>(cpu_caps | ram_caps);
    global_initialized = true;

    // Now, populate the capabilities string:
    char *pos = global_capabilities_string;
    char *end = global_capabilities_string + sizeof(global_capabilities_string) - 1;
    pos += std::snprintf(pos, end - pos, "serial");

    // Start with base capability level
    if (global_capabilities & fu::capability_numa_aware_k) pos += std::snprintf(pos, end - pos, "+numa");
    if (global_capabilities & fu::capability_huge_pages_k) pos += std::snprintf(pos, end - pos, "+hp");
    if (global_capabilities & fu::capability_huge_pages_transparent_k) pos += std::snprintf(pos, end - pos, "+thp");

    // Add CPU-specific extensions
    if (global_capabilities & fu::capability_x86_pause_k) pos += std::snprintf(pos, end - pos, "+x86_pause");
    if (global_capabilities & fu::capability_x86_tpause_k) pos += std::snprintf(pos, end - pos, "+x86_tpause");
    if (global_capabilities & fu::capability_arm64_yield_k) pos += std::snprintf(pos, end - pos, "+arm64_yield");
    if (global_capabilities & fu::capability_arm64_wfet_k) pos += std::snprintf(pos, end - pos, "+arm64_wfet");
    if (global_capabilities & fu::capability_risc5_pause_k) pos += std::snprintf(pos, end - pos, "+risc5_pause");
    return true;
}

extern "C" {

int fu_version_major(void) { return FORK_UNION_VERSION_MAJOR; }
int fu_version_minor(void) { return FORK_UNION_VERSION_MINOR; }
int fu_version_patch(void) { return FORK_UNION_VERSION_PATCH; }
int fu_enabled_numa(void) { return FU_ENABLE_NUMA; }

#pragma region - Metadata

char const *fu_capabilities_string(void) {
    if (!globals_initialize()) return nullptr;
    return &global_capabilities_string[0];
}

size_t fu_count_logical_cores(void) {
#if FU_ENABLE_NUMA
    if (!globals_initialize()) return 0;
    return global_numa_topology.threads_count();
#else
    return std::thread::hardware_concurrency();
#endif
}

size_t fu_count_colocations(void) {
#if FU_ENABLE_NUMA
    if (!globals_initialize()) return 0;
    return global_numa_topology.nodes_count();
#else
    return 1;
#endif
}

size_t fu_count_numa_nodes(void) {
#if FU_ENABLE_NUMA
    if (!globals_initialize()) return 0;
    return global_numa_topology.nodes_count();
#else
    return 1;
#endif
}

size_t fu_count_quality_levels(void) {
    if (!globals_initialize()) return 0;
    return 1; // TODO: One day I'll get some of those weird CPUs to do this
}

size_t fu_volume_any_pages(void) { return fu::get_ram_total_volume(); }

size_t fu_volume_huge_pages_in(size_t numa_node_index) {
#if FU_ENABLE_NUMA
    size_t total_volume = 0;
    auto const &node = global_numa_topology.node(numa_node_index);
    for (auto const &page_size : node.page_sizes) total_volume += page_size.bytes_per_page * page_size.free_pages;
    return total_volume;
#else
    return 0;
#endif
}

size_t fu_volume_any_pages_in(size_t numa_node_index) {
#if FU_ENABLE_NUMA
    if (!globals_initialize()) return 0;
    if (numa_node_index >= global_numa_topology.nodes_count()) return 0;

    auto const &node = global_numa_topology.node(numa_node_index);
    return node.memory_size;
#else
    return fu::get_ram_total_volume();
#endif
}

#pragma endregion - Metadata

#pragma region - Memory

void *fu_allocate_at_least(                       //
    size_t numa_node_index, size_t minimum_bytes, //
    size_t *allocated_bytes, size_t *bytes_per_page) {

#if FU_ENABLE_NUMA
    auto const &node = global_numa_topology.node(numa_node_index);
    fu::linux_numa_allocator_t allocator(node.node_id);
    auto result = allocator.allocate_at_least(minimum_bytes);
    if (!result) return nullptr;
    *allocated_bytes = result.count;
    *bytes_per_page = result.bytes_per_page();
    return result.ptr;
#else
    auto result = std::malloc(minimum_bytes);
    if (!result) return nullptr;
    *allocated_bytes = minimum_bytes;
    *bytes_per_page = fu::get_ram_page_size();
    return result;
#endif
}

void *fu_allocate(size_t numa_node_index, size_t bytes) {

#if FU_ENABLE_NUMA
    auto const &node = global_numa_topology.node(numa_node_index);
    fu::linux_numa_allocator_t allocator(node.node_id);
    return allocator.allocate(bytes);
#else
    return std::malloc(bytes);
#endif
}

void fu_free(size_t numa_node_index, void *pointer, size_t bytes) {
#if FU_ENABLE_NUMA
    auto const &node = global_numa_topology.node(numa_node_index);
    fu::linux_numa_allocator_t allocator(node.node_id);
    allocator.deallocate(reinterpret_cast<char *>(pointer), bytes);
#else
    std::free(pointer);
#endif
}

#pragma endregion - Memory

#pragma region - Lifetime

fu_pool_t *fu_pool_new(char const *name) {
    if (!globals_initialize()) return nullptr;

    opaque_pool_t *opaque = static_cast<opaque_pool_t *>(std::malloc(sizeof(opaque_pool_t)));
    if (!opaque) return nullptr;

    // Best case, use the NUMA-aware distributed pool
#if FU_ENABLE_NUMA
    fu::numa_topology_t copied_topology;
    if (!copied_topology.try_assign(global_numa_topology)) {
        std::free(opaque);
        return nullptr;
    }

#if _FU_WITH_ASM_YIELDS
#if _FU_DETECT_ARCH_X86_64
    if (global_capabilities & fu::capability_x86_tpause_k) {
        new (opaque) opaque_pool_t(std::in_place_type<fu::linux_distributed_pool<fu::x86_tpause_t>>, name,
                                   std::move(copied_topology));
        return reinterpret_cast<fu_pool_t *>(opaque);
    }
    if (global_capabilities & fu::capability_x86_pause_k) {
        new (opaque) opaque_pool_t(std::in_place_type<fu::linux_distributed_pool<fu::x86_pause_t>>, name,
                                   std::move(copied_topology));
        return reinterpret_cast<fu_pool_t *>(opaque);
    }
#endif
#if _FU_DETECT_ARCH_ARM64
    if (global_capabilities & fu::capability_arm64_wfet_k) {
        new (opaque) opaque_pool_t(std::in_place_type<fu::linux_distributed_pool<fu::arm64_wfet_t>>, name,
                                   std::move(copied_topology));
        return reinterpret_cast<fu_pool_t *>(opaque);
    }
    if (global_capabilities & fu::capability_arm64_yield_k) {
        new (opaque) opaque_pool_t(std::in_place_type<fu::linux_distributed_pool<fu::arm64_yield_t>>, name,
                                   std::move(copied_topology));
        return reinterpret_cast<fu_pool_t *>(opaque);
    }
#endif
#if _FU_DETECT_ARCH_RISC5
    if (global_capabilities & fu::capability_risc5_pause_k) {
        new (opaque) opaque_pool_t(std::in_place_type<fu::linux_distributed_pool<fu::risc5_pause_t>>, name,
                                   std::move(copied_topology));
        return reinterpret_cast<fu_pool_t *>(opaque);
    }
#endif
#endif // _FU_WITH_ASM_YIELDS
#endif // FU_ENABLE_NUMA

    // Common case of using modern hardware, but not having Linux installed
#if _FU_WITH_ASM_YIELDS
#if _FU_DETECT_ARCH_X86_64
    if (global_capabilities & fu::capability_x86_tpause_k) {
        new (opaque) opaque_pool_t(std::in_place_type<fu::basic_pool<thread_allocator_t, fu::x86_tpause_t>>);
        return reinterpret_cast<fu_pool_t *>(opaque);
    }
    if (global_capabilities & fu::capability_x86_pause_k) {
        new (opaque) opaque_pool_t(std::in_place_type<fu::basic_pool<thread_allocator_t, fu::x86_pause_t>>);
        return reinterpret_cast<fu_pool_t *>(opaque);
    }
#endif
#if _FU_DETECT_ARCH_ARM64
    if (global_capabilities & fu::capability_arm64_wfet_k) {
        new (opaque) opaque_pool_t(std::in_place_type<fu::basic_pool<thread_allocator_t, fu::arm64_wfet_t>>);
        return reinterpret_cast<fu_pool_t *>(opaque);
    }
    if (global_capabilities & fu::capability_arm64_yield_k) {
        new (opaque) opaque_pool_t(std::in_place_type<fu::basic_pool<thread_allocator_t, fu::arm64_yield_t>>);
        return reinterpret_cast<fu_pool_t *>(opaque);
    }
#endif
#if _FU_DETECT_ARCH_RISC5
    if (global_capabilities & fu::capability_risc5_pause_k) {
        new (opaque) opaque_pool_t(std::in_place_type<fu::basic_pool<thread_allocator_t, fu::risc5_pause_t>>);
        return reinterpret_cast<fu_pool_t *>(opaque);
    }
#endif
#endif // _FU_WITH_ASM_YIELDS

    // Worst case, use the standard yield pool
    new (opaque) opaque_pool_t(std::in_place_type<fu::basic_pool<thread_allocator_t, fu::standard_yield_t>>);
    return reinterpret_cast<fu_pool_t *>(opaque);
}

void fu_pool_delete(fu_pool_t *pool) {
    assert(pool != nullptr);

    opaque_pool_t *opaque = reinterpret_cast<opaque_pool_t *>(pool);
    std::visit([](auto &variant) { variant.terminate(); }, opaque->variants);

    // Call the object's destructor and deallocate the memory
    opaque->~opaque_pool_t();
    std::free(opaque);
}

fu_bool_t fu_pool_spawn(fu_pool_t *pool, size_t threads, fu_caller_exclusivity_t c_exclusivity) {
    assert(pool != nullptr);
    assert(c_exclusivity == fu_caller_inclusive_k || c_exclusivity == fu_caller_exclusive_k);
    opaque_pool_t *opaque = reinterpret_cast<opaque_pool_t *>(pool);
    auto exclusivity = c_exclusivity == fu_caller_inclusive_k ? fu::caller_inclusive_k : fu::caller_exclusive_k;
    return std::visit([=](auto &variant) { return variant.try_spawn(threads, exclusivity); }, opaque->variants);
}

void fu_pool_sleep(fu_pool_t *pool, size_t micros) {
    assert(pool != nullptr);
    opaque_pool_t *opaque = reinterpret_cast<opaque_pool_t *>(pool);
    std::visit([=](auto &variant) { variant.sleep(micros); }, opaque->variants);
}

void fu_pool_terminate(fu_pool_t *pool) {
    assert(pool != nullptr);
    opaque_pool_t *opaque = reinterpret_cast<opaque_pool_t *>(pool);
    std::visit([](auto &variant) { variant.terminate(); }, opaque->variants);
}

size_t fu_pool_count_colocations(fu_pool_t *pool) {
    assert(pool != nullptr);
    opaque_pool_t *opaque = reinterpret_cast<opaque_pool_t *>(pool);
    return std::visit([](auto &variant) { return variant.colocations_count(); }, opaque->variants);
}

size_t fu_pool_count_threads(fu_pool_t *pool) {
    assert(pool != nullptr);
    opaque_pool_t *opaque = reinterpret_cast<opaque_pool_t *>(pool);
    return std::visit([](auto &variant) { return variant.threads_count(); }, opaque->variants);
}

size_t fu_pool_count_threads_in(fu_pool_t *pool, size_t colocation_index) {
    assert(pool != nullptr);
    opaque_pool_t *opaque = reinterpret_cast<opaque_pool_t *>(pool);
    return std::visit([=](auto &variant) { return variant.threads_count(colocation_index); }, opaque->variants);
}

size_t fu_pool_locate_thread_in(fu_pool_t *pool, size_t global_thread_index, size_t colocation_index) {
    assert(pool != nullptr);
    opaque_pool_t *opaque = reinterpret_cast<opaque_pool_t *>(pool);
    return std::visit([=](auto &variant) { return variant.thread_local_index(global_thread_index, colocation_index); },
                      opaque->variants);
}

#pragma endregion - Lifetime

#pragma region - Primary API

void fu_pool_for_threads(fu_pool_t *pool, fu_for_threads_t callback, fu_lambda_context_t context) {
    assert(pool != nullptr && callback != nullptr);
    opaque_pool_t *opaque = reinterpret_cast<opaque_pool_t *>(pool);
    std::visit(
        [&](auto &variant) {
            variant.for_threads([=](fu::colocated_thread_t pinned) noexcept { //
                callback(context, pinned.thread, pinned.colocation);
            });
        },
        opaque->variants);
}

void fu_pool_for_n(fu_pool_t *pool, size_t n, fu_for_prongs_t callback, fu_lambda_context_t context) {
    assert(pool != nullptr && callback != nullptr);
    opaque_pool_t *opaque = reinterpret_cast<opaque_pool_t *>(pool);
    std::visit(
        [&](auto &variant) {
            variant.for_n(n, [=](fu::colocated_prong_t prong) noexcept { //
                callback(context, prong.task, prong.thread, prong.colocation);
            });
        },
        opaque->variants);
}

void fu_pool_for_n_dynamic(fu_pool_t *pool, size_t n, fu_for_prongs_t callback, fu_lambda_context_t context) {
    assert(pool != nullptr && callback != nullptr);
    opaque_pool_t *opaque = reinterpret_cast<opaque_pool_t *>(pool);
    std::visit(
        [&](auto &variant) {
            variant.for_n_dynamic(n, [=](fu::colocated_prong_t prong) noexcept { //
                callback(context, prong.task, prong.thread, prong.colocation);
            });
        },
        opaque->variants);
}

void fu_pool_for_slices(fu_pool_t *pool, size_t n, fu_for_slices_t callback, fu_lambda_context_t context) {
    assert(pool != nullptr && callback != nullptr);
    opaque_pool_t *opaque = reinterpret_cast<opaque_pool_t *>(pool);
    std::visit(
        [&](auto &variant) {
            variant.for_slices(n, [=](fu::colocated_prong_t prong, std::size_t count) noexcept { //
                callback(context, prong.task, count, prong.thread, prong.colocation);
            });
        },
        opaque->variants);
}

#pragma endregion - Primary API

#pragma region - Flexible API

void fu_pool_unsafe_for_threads(fu_pool_t *pool, fu_for_threads_t callback, fu_lambda_context_t context) {
    assert(pool != nullptr && callback != nullptr);
    opaque_pool_t *opaque = reinterpret_cast<opaque_pool_t *>(pool);
    opaque->current_context = context;
    opaque->current_callback = callback;
    std::visit([&](auto &variant) { variant.unsafe_for_threads(*opaque); }, opaque->variants);
}

void fu_pool_unsafe_join(fu_pool_t *pool) {
    assert(pool != nullptr);
    opaque_pool_t *opaque = reinterpret_cast<opaque_pool_t *>(pool);
    assert(opaque->current_context != nullptr);
    std::visit([](auto &variant) { variant.unsafe_join(); }, opaque->variants);
    opaque->current_context = nullptr;
    opaque->current_callback = nullptr;
}

#pragma endregion - Flexible API
}
