#include "mpi_util.hpp"
#ifdef USE_MPI
#include <mpi.h>
#define OMPI_SKIP_MPICXX 1 // Disable MPI-C++ bindings
#endif
#include <iostream>
#include <numeric>
#include <omp.h>

using namespace std;

namespace MPIUtil {

#ifdef USE_MPI
  const MPI_Comm MPICommunicator = MPI_COMM_WORLD;
#endif

  void init() {
#ifdef USE_MPI
    MPI_Init(nullptr, nullptr);
#endif
  }

  void finalize() {
#ifdef USE_MPI
    MPI_Finalize();
#endif
  }

  bool isInitialized() {
#ifdef USE_MPI
    int isMPIInit;
    MPI_Initialized(&isMPIInit);
    return isMPIInit == 1;
#endif
    return true;
  }

  int rank() {
#ifdef USE_MPI
    int rank;
    MPI_Comm_rank(MPICommunicator, &rank);
    return rank;
#endif
    return 0;
  }

  int numberOfRanks() {
#ifdef USE_MPI
    int numRanks;
    MPI_Comm_size(MPICommunicator, &numRanks);
    return numRanks;
#endif
    return 1;
  }

  void barrier() {
#ifdef USE_MPI
    MPI_Barrier(MPICommunicator);
#endif
  }

  bool isRoot() {
#ifdef USE_MPI
    if (isInitialized()) {
      return rank() == 0;
    } else {
      std::cout << "Warning: MPI method isRoot called before MPI was "
                   "initialized. It will default to true."
                << std::endl;
      return true;
    }
#endif
    return true;
  }

  bool isSingleProcess() {
#ifdef USE_MPI
    if (isInitialized()) {
      return numberOfRanks() == 1;
    } else {
      std::cout << "Warning: MPI method isSingleProcess called before MPI was "
                   "initialized. It will default to true."
                << std::endl;
      return true;
    }
#endif
    return true;
  }

  void throwError(const string &errMsg) {
    if (isSingleProcess()) {
      // Throw a catchable error if only one process is used
      throw runtime_error(errMsg);
    }
#ifdef USE_MPI
    // Abort MPI if more than one process is running
    cerr << errMsg << endl;
    abort();
#endif
  }

  void abort() {
#ifdef USE_MPI
    MPI_Abort(MPICommunicator, 1);
#endif
  }

  double timer() {
#ifdef USE_MPI
    return MPI_Wtime();
#endif
    return omp_get_wtime();
  }

  bool isEqualOnAllRanks(const int &myNumber) {
#ifdef USE_MPI
    int globalMininumNumber;
    MPI_Allreduce(
        &myNumber, &globalMininumNumber, 1, MPI_INT, MPI_MIN, MPICommunicator);
    return myNumber == globalMininumNumber;
#endif
    (void)myNumber;
    return true;
  }

  pair<int, int> getLoopIndexes(const int loopSize, const int thisRank) {
    pair<int, int> idx = {0, loopSize};
    const int nRanks = numberOfRanks();
    if (nRanks == 1) { return idx; }
    int localSize = loopSize / nRanks;
    int remainder = loopSize % nRanks;
    idx.first = thisRank * localSize + std::min(thisRank, remainder);
    idx.second = idx.first + localSize + (thisRank < remainder ? 1 : 0);
    idx.second = std::min(idx.second, loopSize);
    return idx;
  }

  MPIParallelForData getAllLoopIndexes(const int loopSize) {
    std::vector<pair<int, int>> out;
    for (int i = 0; i < numberOfRanks(); ++i) {
      out.push_back(getLoopIndexes(loopSize, i));
    }
    return out;
  }

  MPIParallelForData parallelFor(const function<void(int)> &loopFunc,
                                 const int loopSize,
                                 const int ompThreads) {
    MPIParallelForData allIdx = getAllLoopIndexes(loopSize);
    const auto &thisIdx = allIdx[rank()];
    const bool useOMP = ompThreads > 1;
#pragma omp parallel for num_threads(ompThreads) if (useOMP)
    for (int i = thisIdx.first; i < thisIdx.second; ++i) {
      loopFunc(i);
    }
    return allIdx;
  }

  void gatherLoopData(double *dataToGather,
                      const MPIParallelForData &loopData,
                      const int countsPerLoop) {
#ifdef USE_MPI
    std::vector<int> recieverCounts;
    for (const auto &i : loopData) {
      const int loopSpan = i.second - i.first;
      recieverCounts.push_back(loopSpan * countsPerLoop);
    }
    std::vector<int> displacements(recieverCounts.size(), 0);
    std::partial_sum(recieverCounts.begin(),
                     recieverCounts.end() - 1,
                     displacements.begin() + 1,
                     plus<double>());
    MPI_Allgatherv(MPI_IN_PLACE,
                   0,
                   MPI_DATATYPE_NULL,
                   dataToGather,
                   recieverCounts.data(),
                   displacements.data(),
                   MPI_DOUBLE,
                   MPI_COMM_WORLD);
#endif
    if (!dataToGather) { throwError(""); }
    (void)loopData;
    (void)countsPerLoop;
  }

} // namespace MPIUtil
